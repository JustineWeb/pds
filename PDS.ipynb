{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MY NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import itertools\n",
    "\n",
    "import progressbar\n",
    "from time import sleep\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "MTT_DIR = \"../MTT/\"\n",
    "DATA_DIRECTORY = MTT_DIR + \"dataset/\"\n",
    "# CSV containing song details (title, artist, id, mp3_path,...)\n",
    "CLIP_INFO_FINAL = \"clip_info_final.csv\"\n",
    "# CSV containg the labels (clip_id, labels, mp3_path)\n",
    "ANNOTATIONS_FINAL = \"annotations_final.csv\"\n",
    "# CSV : what is it useful for ?\n",
    "COMPARISONS_FINAL = \"comparisons_final.csv\"\n",
    "LABELS_FILE = MTT_DIR + ANNOTATIONS_FINAL\n",
    "\n",
    "SUB_DIRS = \"0123456789abcdef\"\n",
    "\n",
    "AUDIO1_path = \"../MTT/mtt_data_mp3.zip/0/american_bach_soloists-j_s__bach_\\\n",
    "_transcriptions_of_italian_music-02-concerto_in_a_minor_for_four_harpsichords\\\n",
    "_bwv_1065_ii_largo-88-117.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NB = 25880 #25863\n",
    "FILE_LENGTH = 465984\n",
    "# 465984 = 2 × 2 × 2 × 2 × 2 × 2 × 3 × 3 × 809\n",
    "# useful for batches > for now divide by 9 (instead of 10)\n",
    "\n",
    "BATCH_NB = 9\n",
    "BATCH_SIZE = int(FILE_LENGTH/BATCH_NB)\n",
    "SAMPLE_SIZE = 0\n",
    "SAMPLE_RATE = 16000\n",
    "RECEPTIVE_FIELD = 0\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "BASIC_CONFIG ={'numOutputNeurons':500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(labels_file_name):\n",
    "    pd.read_csv(labels_file_name)\n",
    "\n",
    "#def load_1label(labels_file_name):\n",
    " #   data = pd.read_csv(labels_file_name)\n",
    "  #  print(data.head())\n",
    "\n",
    "def randomize_files(files):\n",
    "    for file in files:\n",
    "        file_index = random.randint(0, (len(files) - 1))\n",
    "        yield files[file_index]\n",
    "        \n",
    "def load_generic_audio(directory, sample_rate):\n",
    "    '''Generator that yields audio waveforms from the directory.'''\n",
    "    files = find_files(directory)\n",
    "    # format of content of files :\n",
    "    # <../MTT/dataset/9/the_kokoon-berlin-07-sm_art-146-175.mp3>\n",
    "    # files[i][len(DATA_DIRECTORY):] extract the part corresponding\n",
    "    # to mp3_path in annotations csv :  <9/the_kokoon-berlin-07-sm_art-146-175.mp3>\n",
    "\n",
    " #   x = load_1label(LABELS_FILE)\n",
    "  #  return \n",
    "    #print(\"files length: {}\".format(len(files)))\n",
    "   # randomized_files = randomize_files(files)\n",
    "    count = 0\n",
    "   # for filename in randomized_files:\n",
    "    for filename in files:\n",
    "\n",
    "        try :\n",
    "            audio, _ = librosa.load(filename, sr=sample_rate, mono=True)\n",
    "            audio = audio.reshape(-1, 1)\n",
    "            count +=1\n",
    "            yield audio, filename, count\n",
    "        except EOFError :\n",
    "            print(\"EOFERROR : The following file could not be loaded with librosa - \", filename)\n",
    "\n",
    "\n",
    "\n",
    "def find_files(directory, pattern='*.mp3', sample=None):\n",
    "    '''Recursively finds all files matching the pattern.'''\n",
    "    files = []\n",
    "    for root, dirnames, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            files.append(os.path.join(root, filename))\n",
    "    if sample!=None :\n",
    "        try:\n",
    "            return files[:sample]\n",
    "        except TypeError:\n",
    "            print(\"Argument sample should be either None, or an integer :\\\n",
    "             the number of first n samples to take.\")\n",
    "    else :\n",
    "        return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean labels for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv(LABELS_FILE, sep = '\"\\t\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare header to put back in the end\n",
    "# remove quotes and take all columns except the first one\n",
    "header = list(map(lambda x : x.replace('\"', ''), labels))[1:]\n",
    "# add back the first column, separated in two\n",
    "header = ['clip_id', 'no_voice']+header\n",
    "# create dictionary\n",
    "header = dict(enumerate(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_id</th>\n",
       "      <th>no_voice</th>\n",
       "      <th>singer</th>\n",
       "      <th>duet</th>\n",
       "      <th>plucking</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>world</th>\n",
       "      <th>bongos</th>\n",
       "      <th>harpsichord</th>\n",
       "      <th>female singing</th>\n",
       "      <th>...</th>\n",
       "      <th>rap</th>\n",
       "      <th>metal</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>quick</th>\n",
       "      <th>water</th>\n",
       "      <th>baroque</th>\n",
       "      <th>women</th>\n",
       "      <th>fiddle</th>\n",
       "      <th>english</th>\n",
       "      <th>mp3_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  clip_id no_voice singer duet plucking hard rock world bongos harpsichord  \\\n",
       "0       2        0      0    0        0         0     0      0           0   \n",
       "1       6        0      0    0        0         0     0      0           0   \n",
       "2      10        0      0    0        0         0     0      0           0   \n",
       "3      11        0      0    0        0         0     0      0           0   \n",
       "4      12        0      0    0        0         0     0      0           0   \n",
       "\n",
       "  female singing                        ...                         rap metal  \\\n",
       "0              0                        ...                           0     0   \n",
       "1              0                        ...                           0     0   \n",
       "2              0                        ...                           0     0   \n",
       "3              0                        ...                           0     0   \n",
       "4              0                        ...                           0     0   \n",
       "\n",
       "  hip hop quick water baroque women fiddle english  \\\n",
       "0       0     0     0       0     0      0       0   \n",
       "1       0     0     0       1     0      0       0   \n",
       "2       0     0     0       0     0      0       0   \n",
       "3       0     0     0       0     0      0       0   \n",
       "4       0     0     0       0     0      0       0   \n",
       "\n",
       "                                            mp3_path  \n",
       "0  f/american_bach_soloists-j_s__bach_solo_cantat...  \n",
       "1  f/american_bach_soloists-j_s__bach_solo_cantat...  \n",
       "2  f/american_bach_soloists-j_s__bach_solo_cantat...  \n",
       "3  f/american_bach_soloists-j_s__bach_solo_cantat...  \n",
       "4  f/american_bach_soloists-j_s__bach_solo_cantat...  \n",
       "\n",
       "[5 rows x 190 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solve format problem : two first columns are merged\n",
    "# extract first column and rest\n",
    "left, right = labels['\"clip_id\\t\"\"no voice\"'], labels.iloc[:, 1:]\n",
    "# split first column in two part at separator \"\\t\"\n",
    "split = left.str.split(pat = \"\\t\", expand=True).replace('\"', '')\n",
    "\n",
    "# put back the first column which is now two, with the rest\n",
    "cleaned = pd.concat([split, right], axis=1, ignore_index=True) \n",
    "# clean by removing quotes and add back header\n",
    "cleaned = cleaned.apply(lambda col : col.apply(lambda x : x.replace('\"', ''))).rename(columns = header)\n",
    "cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Some statistics on the MTT dataset ?\n",
    "nb_labels_per_song = cleaned.iloc[:,1:-1].astype(int).sum(axis=1)\n",
    "nb_song_per_label = cleaned.iloc[:,1:-1].astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs :  25863\n",
      "Number of labels :  190\n",
      "Max number of songs tagged with the same label :  4852\n",
      "Max number of labels for a single song :  27\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of songs : \" , cleaned.shape[0])\n",
    "print(\"Number of labels : \" , cleaned.shape[1])\n",
    "print(\"Max number of songs tagged with the same label : \",max(nb_song_per_label))\n",
    "print(\"Max number of labels for a single song : \",max(nb_labels_per_song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(nb_labels_per_song)#, bins = [0,20,40,60,80,100]) \n",
    "#plt.title(\"histogram\") \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25863, 4)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_labels = cleaned[['clip_id', 'no_voice', 'singer', 'mp3_path']]\n",
    "sample_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and format mp3 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = load_generic_audio(DATA_DIRECTORY, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : not efficient to \n",
    "# - give the whole label dataset to the function as argument\n",
    "# - look in the label dataset at each iteration to retrieve the label\n",
    "#   > should concat before randomize\n",
    "# - append to an array and then convert to numpy ? > check\n",
    "\n",
    "def load_audio_label(labels, directory, sample_rate, num_songs):\n",
    "    files = find_files(directory, sample=num_songs)\n",
    "        # format of content of files :\n",
    "        # <../MTT/dataset/9/the_kokoon-berlin-07-sm_art-146-175.mp3>\n",
    "        # files[i][len(DATA_DIRECTORY):] extract the part corresponding\n",
    "        # to mp3_path in annotations csv :  <9/the_kokoon-berlin-07-sm_art-146-175.mp3>\n",
    "\n",
    "     #   x = load_1label(LABELS_FILE)\n",
    "      #  return \n",
    "        #print(\"files length: {}\".format(len(files)))\n",
    "    randomized_files = randomize_files(files)\n",
    "    count = 0\n",
    "    audios = np.ndarray(shape=(num_songs, BATCH_NB, BATCH_SIZE, 1), dtype=np.float32, order='F')\n",
    "    tags = np.ndarray(shape=(num_songs, BATCH_NB, labels.shape[1]-2, 1), dtype=np.float32, order='F')\n",
    "    \n",
    "    for filename in randomized_files:\n",
    "    #    for filename in files:\n",
    "\n",
    "        #if count > 200 :\n",
    "         #   return\n",
    "        \n",
    "        try :\n",
    "            audio, _ = librosa.load(filename, sr=sample_rate, mono=True)\n",
    "        except EOFError :\n",
    "            print(\"EOFERROR : The following file could not be loaded with librosa - \", filename)\n",
    "        #print(audio)\n",
    "        audio = audio.reshape(-1, 1)\n",
    "        #print(audio)\n",
    "        #create batches\n",
    "        audio_batch = np.ndarray(shape=(BATCH_NB, BATCH_SIZE, 1), dtype=np.float32, order='F')\n",
    "        label_batch = np.ndarray(shape=(BATCH_NB, labels.shape[1]-2, 1), dtype=np.float32, order='F')\n",
    "        for n in range(BATCH_NB) :\n",
    "            audio_batch[n] = audio[n*BATCH_SIZE: (n+1)*BATCH_SIZE,:]\n",
    "            label_batch[n] = labels.loc[labels['mp3_path']==filename[len(DATA_DIRECTORY):]] \\\n",
    "                               .values[:, 1:-1].reshape(labels.shape[1]-2,1)\n",
    "        #print(len(audio_batch), len(audio_batch[0]), len(audio_batch[0][0]))\n",
    "        \n",
    "        #audios.append(audio_batch) \n",
    "        audios[count] = audio_batch\n",
    "        tags[count] = label_batch\n",
    "        \n",
    "        count +=1\n",
    "        if (count % 100) == 0:\n",
    "            print(count)\n",
    "            #print(\"AUDIO : \", audio)\n",
    "            #print(\"LABELS : \", labels.loc[labels['mp3_path']==filename[len(DATA_DIRECTORY):]])\n",
    "            #print(\"FILENAME :\", filename)\n",
    "         \n",
    "        # TODO : create batches > here or later to rereandomize the order ?\n",
    "\n",
    "        #print()\n",
    "        \n",
    "        \n",
    "    return audios, tags\n",
    "        #yield audio, labels, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load audios and labels > convert to numpy\n",
    "# CAREFUL : the argument num_songs is important and shouldn't be too big \n",
    "# > otherwise MEMORY ISSUES !!!!!!\n",
    "audios_sample, tags_sample = load_audio_label(sample_labels, DATA_DIRECTORY, None, num_songs = 3)\n",
    "#audios_sample = np.asarray(audios_sample)\n",
    "#tags_sample = np.asarray(tags_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of audios list :\n",
      "(3, 9, 51776, 1)\n",
      "\n",
      ">> shape is : [num_song, num_batches, batch_size, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of audios list :\")\n",
    "print(audios_sample.shape)\n",
    "print()\n",
    "print(\">> shape is : [num_song, num_batches, batch_size, 1]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tags list :\n",
      "(3, 9, 2, 1)\n",
      "\n",
      ">> shape is : [num_song, num_labels (mp3 file and id excluded), 1] \n",
      "(or : [num_song, num_labels, 1, str_len, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of tags list :\")\n",
    "print(tags_sample.shape)\n",
    "print()\n",
    "print(\">> shape is : [num_song, num_labels (mp3 file and id excluded), 1] \")\n",
    "print(\"(or : [num_song, num_labels, 1, str_len, 1])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all batches at the same level (remove song dimension in the array)\n",
    "# ex : go from dimensions (3, 9, 51776, 1) > to (27, 51776, 1)\n",
    "audios_sample_flatten = audios_sample.reshape(-1, audios_sample.shape[-2], audios_sample.shape[-1])\n",
    "tags_sample_flatten = tags_sample.reshape(-1, tags_sample.shape[-2], tags_sample.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 1)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voice_no_voice = tags_sample_flatten[:,1,:]\n",
    "voice_no_voice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE FROM TOWARDS DATASCIENCE\n",
    "    #EPOCHS = 10\n",
    "    #BATCH_SIZE = 16\n",
    "    ## using two numpy arrays\n",
    "    #features, labels = (np.array([np.random.sample((100,2))]), \n",
    "    #                    np.array([np.random.sample((100,1))]))\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)\n",
    "    #iter = dataset.make_one_shot_iterator()\n",
    "    #x, y = iter.get_next()\n",
    "    ## make a simple model\n",
    "    #net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "    #net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "    #prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "    #loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\n",
    "    #train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "    #with tf.Session() as sess:\n",
    "    #    sess.run(tf.global_variables_initializer())\n",
    "    #    for i in range(EPOCHS):\n",
    "    #        _, loss_value = sess.run([train_op, loss])\n",
    "    #        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 1)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voice_no_voice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 51776, 1)\n",
      "Iter: 0, Loss: 1.0258\n",
      "Iter: 1, Loss: 0.9866\n",
      "Iter: 2, Loss: 0.9305\n",
      "Iter: 3, Loss: 0.9045\n",
      "Iter: 4, Loss: 0.8574\n",
      "Iter: 5, Loss: 0.8421\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-8584cec217dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m#print(loss_value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iter: {}, Loss: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "audio_tf= tf.convert_to_tensor(audios_sample_flatten, np.float32)\n",
    "print(audio_tf.shape)\n",
    "\n",
    "\n",
    "\n",
    "net = build_model(audio_tf, is_training=True, config=BASIC_CONFIG) \n",
    "prediction = tf.layers.dense(net, 1, activation=tf.sigmoid)\n",
    "#loss1 = tf.losses.mean_squared_error(prediction, voice_no_voice)\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = voice_no_voice, logits = prediction)\n",
    "reduced_loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(reduced_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(EPOCHS):\n",
    "        _, loss_value = sess.run([train_op, reduced_loss])\n",
    "        #print(loss_value)\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell everytime before relaunching tensorflow session\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
